{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "from nltk.util import ngrams\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lemmatized_tweets.pkl\",\"rb\") as file_handle:\n",
    "    data = pickle.load(file_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.apply(lambda x: \" \".join(x)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_raw_text = list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_raw_text = list(filter(lambda x: len(x) > 1,converted_raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"truncated-cleaned-tweets.txt\",\"wt\") as file_handle:\n",
    "\n",
    "    file_handle.writelines(truncated_raw_text[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = datasets.load_dataset(\"text\",data_files={\"train\":\"truncated-cleaned-tweets.txt\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweets(single_row):\n",
    "\n",
    "    single_row[\"tokenized-tweets\"] = single_row[\"text\"].split()\n",
    "    return single_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset[\"train\"] = dset[\"train\"].map(tokenize_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_trigrams(single_row):\n",
    "\n",
    "    single_row[\"tri-grams\"] = list(ngrams(single_row[\"tokenized-tweets\"],n=3))\n",
    "    return single_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset[\"train\"] = dset[\"train\"].map(convert_to_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "\n",
    "for raw_text in truncated_raw_text[:100000]:\n",
    "    vocabulary.update(raw_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2idx = dict(zip(vocabulary,range(len(vocabulary))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bigrams(single_row):\n",
    "\n",
    "    center_token_target_token_pairs = list()\n",
    "\n",
    "    for single_trigram in single_row[\"tri-grams\"]:\n",
    "\n",
    "        bigrams = list()\n",
    "        \n",
    "        bigrams.append([vocab2idx[single_trigram[1]],\n",
    "                                                vocab2idx[single_trigram[0]]])\n",
    "        bigrams.append([vocab2idx[single_trigram[1]],\n",
    "                                                vocab2idx[single_trigram[2]]])\n",
    "        center_token_target_token_pairs.append(bigrams)\n",
    "\n",
    "    single_row[\"tri-grams\"] = center_token_target_token_pairs\n",
    "\n",
    "    return single_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset[\"train\"] = dset[\"train\"].map(convert_to_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_target_token_pairs = list()\n",
    "\n",
    "for single_tweet_bigrams in dset[\"train\"][\"tri-grams\"]:\n",
    "    for bigrams_list in single_tweet_bigrams:\n",
    "\n",
    "        input_token_target_token_pairs.append(bigrams_list[0])\n",
    "        input_token_target_token_pairs.append(bigrams_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self,input_target_pairs):\n",
    "        self.data = input_target_pairs\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_obj = SkipGramDataset(input_token_target_token_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_generator = torch.utils.data.DataLoader(training_data_obj,batch_size=32,\n",
    "                                                     num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecSkipGramNeuralNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,vocabulary_size,topic_vector_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_layer = torch.nn.Embedding(num_embeddings=vocabulary_size,\n",
    "                                               embedding_dim=topic_vector_dim)\n",
    "        self.output_layer = torch.nn.Linear(in_features=topic_vector_dim,\n",
    "                                            out_features=vocabulary_size)\n",
    "        self.output_layer_activation = torch.nn.Softmax()\n",
    "\n",
    "    \n",
    "    def forward(self,center_token):\n",
    "\n",
    "        embedding_layer_out = self.hidden_layer(center_token)\n",
    "        linear_layer_out = self.output_layer(embedding_layer_out)\n",
    "        nn_out = self.output_layer_activation(linear_layer_out)\n",
    "\n",
    "        return nn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_word2vec_skip_gram_nw = Word2VecSkipGramNeuralNetwork(len(vocab2idx),64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_word2vec_skip_gram_nw.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mini_batch_idx, mini_batch in enumerate(training_data_generator):\n",
    "\n",
    "    print(\"Index of Mini Batch is {}\".format(mini_batch_idx))\n",
    "    print(\"Center Token Mini Batch is {}\".format(mini_batch[0]))\n",
    "    print(\"Surrounding Token Mini Batch is {}\".format(mini_batch[1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "optimizer = torch.optim.Adam(params=our_word2vec_skip_gram_nw.parameters(),\n",
    "                             lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "progress_bar = tqdm(range(epochs * len(training_data_generator)))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for mini_batch_idx, mini_batch in enumerate(training_data_generator):\n",
    "\n",
    "        center_token_mini_batch = mini_batch[0]\n",
    "        surrounding_token_mini_batch = mini_batch[1]\n",
    "\n",
    "        center_token_mini_batch.to(\"cpu\")\n",
    "        surrounding_token_mini_batch.to(\"cpu\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (mini_batch_idx+1) % 1000 == 0:\n",
    "            print(\"Epoch # {}, Time Step # {}, Loss = {}\".format(epoch,(mini_batch_idx+1),\n",
    "                                                             loss_fn_value))\n",
    "\n",
    "        y_pred = our_word2vec_skip_gram_nw(center_token_mini_batch)\n",
    "\n",
    "        loss_fn_value = loss_fn(y_pred,surrounding_token_mini_batch)\n",
    "        loss_fn_value.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
